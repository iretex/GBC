{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://spark.apache.org/docs/latest/api/python/getting_started/quickstart_df.html\n\n[Spark](https://www.apache.org/dyn/closer.lua/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz)\n\n\n[Hadoop](https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.3.6/hadoop-3.3.6-src.tar.gz)\n\nset HADOOP_HOME var to /hadoop-common/src/main/\n\n[Winutils](https://github.com/steveloughran/winutils)\n\ncp winutils.exe to $HADOOP_HOME/bin\n\n[MongoDb](https://www.mongodb.com/docs/spark-connector/v3.0/python-api/)","metadata":{}},{"cell_type":"code","source":"pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2023-07-14T19:54:22.512334Z","iopub.execute_input":"2023-07-14T19:54:22.512741Z","iopub.status.idle":"2023-07-14T19:55:16.591728Z","shell.execute_reply.started":"2023-07-14T19:54:22.512708Z","shell.execute_reply":"2023-07-14T19:55:16.590235Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285413 sha256=66af1cf5d72f4604aed01dbff2699fc2c5e5fe97f18820236e750abd75496e66\n  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\nSuccessfully built pyspark\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.4.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"# !python -m pip install pymongo\n\nfrom pymongo.mongo_client import MongoClient\nfrom pymongo.server_api import ServerApi\n\nuri = \"mongodb+srv://iretioluwaolawuyi:7KoupjCPhO5uZwVC@cluster0.spixfoc.mongodb.net/?retryWrites=true&w=majority\"\n\n# Create a new client and connect to the server\nclient = MongoClient(uri, server_api=ServerApi('1'))\n\n# Send a ping to confirm a successful connection\ntry:\n    client.admin.command('ping')\n    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\nexcept Exception as e:\n    print(e)","metadata":{},"execution_count":1,"outputs":[{"name":"stdout","output_type":"stream","text":"Pinged your deployment. You successfully connected to MongoDB!\n"}]},{"cell_type":"code","source":"import os\nimport sys\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n# os.environ['HADOOP_HOME'] = r\"C:\\Users\\iolawuyi\\Downloads\\spark-3.4.1-bin-hadoop3\\spark-3.4.1-bin-hadoop3\\bin\"\n# os.environ['HADOOP_HOME'] = r\"C:\\Users\\iolawuyi\\Downloads\\hadoop-3.3.6-src.tar\\hadoop-3.3.6-src\\hadoop-common-project\\hadoop-common\\src\\main\"\nos.environ['HADOOP_HOME'] = r\"C:/Users/iolawuyi/Downloads/hadoop-3.3.6-src.tar/hadoop-3.3.6-src/hadoop-common-project/hadoop-common/src/main\"\nos.environ['SPARK_HOME'] = \"C:/Users/iolawuyi/Downloads/spark-3.4.1-bin-hadoop3/spark-3.4.1-bin-hadoop3/bin\"\n","metadata":{},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as f\nimport os\n\n# Set the required configuration properties\n# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 pyspark-shell'\n# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 pyspark-shell'\n\n# .config(\"spark.hadoop.home.dir\", \"C:/Users/iolawuyi/Downloads/spark-3.4.1-bin-hadoop3/spark-3.4.1-bin-hadoop3/bin\") \\\n\n# spark = (SparkSession.builder.getOrCreate())\nspark = SparkSession.builder \\\n    .master(\"local[*]\") \\\n    .config(\"spark.driver.memory\", \"16G\") \\\n    .getOrCreate()","metadata":{},"execution_count":2,"outputs":[{"ename":"FileNotFoundError","evalue":"[WinError 2] The system cannot find the file specified","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[2], line 15\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m# Set the required configuration properties\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39m# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 pyspark-shell'\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 pyspark-shell'\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[39m# spark = (SparkSession.builder.getOrCreate())\u001b[39;00m\n\u001b[0;32m     12\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39;49mbuilder \\\n\u001b[0;32m     13\u001b[0m     \u001b[39m.\u001b[39;49mmaster(\u001b[39m\"\u001b[39;49m\u001b[39mlocal[*]\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[0;32m     14\u001b[0m     \u001b[39m.\u001b[39;49mconfig(\u001b[39m\"\u001b[39;49m\u001b[39mspark.driver.memory\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m16G\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m---> 15\u001b[0m     \u001b[39m.\u001b[39;49mgetOrCreate()\n","File \u001b[1;32mc:\\Users\\iolawuyi\\OneDrive - LinamarCorporation\\Documents\\Learn\\BDTT\\env\\Lib\\site-packages\\pyspark\\sql\\session.py:477\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    475\u001b[0m     sparkConf\u001b[39m.\u001b[39mset(key, value)\n\u001b[0;32m    476\u001b[0m \u001b[39m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 477\u001b[0m sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39;49mgetOrCreate(sparkConf)\n\u001b[0;32m    478\u001b[0m \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    479\u001b[0m \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[0;32m    480\u001b[0m session \u001b[39m=\u001b[39m SparkSession(sc, options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n","File \u001b[1;32mc:\\Users\\iolawuyi\\OneDrive - LinamarCorporation\\Documents\\Learn\\BDTT\\env\\Lib\\site-packages\\pyspark\\context.py:512\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    511\u001b[0m     \u001b[39mif\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 512\u001b[0m         SparkContext(conf\u001b[39m=\u001b[39;49mconf \u001b[39mor\u001b[39;49;00m SparkConf())\n\u001b[0;32m    513\u001b[0m     \u001b[39massert\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    514\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n","File \u001b[1;32mc:\\Users\\iolawuyi\\OneDrive - LinamarCorporation\\Documents\\Learn\\BDTT\\env\\Lib\\site-packages\\pyspark\\context.py:198\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    193\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    194\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    196\u001b[0m     )\n\u001b[1;32m--> 198\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[0;32m    199\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(\n\u001b[0;32m    201\u001b[0m         master,\n\u001b[0;32m    202\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    212\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    213\u001b[0m     )\n","File \u001b[1;32mc:\\Users\\iolawuyi\\OneDrive - LinamarCorporation\\Documents\\Learn\\BDTT\\env\\Lib\\site-packages\\pyspark\\context.py:432\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    431\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_gateway:\n\u001b[1;32m--> 432\u001b[0m         SparkContext\u001b[39m.\u001b[39m_gateway \u001b[39m=\u001b[39m gateway \u001b[39mor\u001b[39;00m launch_gateway(conf)\n\u001b[0;32m    433\u001b[0m         SparkContext\u001b[39m.\u001b[39m_jvm \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_gateway\u001b[39m.\u001b[39mjvm\n\u001b[0;32m    435\u001b[0m     \u001b[39mif\u001b[39;00m instance:\n","File \u001b[1;32mc:\\Users\\iolawuyi\\OneDrive - LinamarCorporation\\Documents\\Learn\\BDTT\\env\\Lib\\site-packages\\pyspark\\java_gateway.py:99\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m     proc \u001b[39m=\u001b[39m Popen(command, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpopen_kwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     98\u001b[0m     \u001b[39m# preexec_fn not supported on Windows\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m     proc \u001b[39m=\u001b[39m Popen(command, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpopen_kwargs)\n\u001b[0;32m    101\u001b[0m \u001b[39m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m proc\u001b[39m.\u001b[39mpoll() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[0;32m   1022\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_mode:\n\u001b[0;32m   1023\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mTextIOWrapper(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr,\n\u001b[0;32m   1024\u001b[0m                     encoding\u001b[39m=\u001b[39mencoding, errors\u001b[39m=\u001b[39merrors)\n\u001b[1;32m-> 1026\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0;32m   1027\u001b[0m                         pass_fds, cwd, env,\n\u001b[0;32m   1028\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[0;32m   1029\u001b[0m                         p2cread, p2cwrite,\n\u001b[0;32m   1030\u001b[0m                         c2pread, c2pwrite,\n\u001b[0;32m   1031\u001b[0m                         errread, errwrite,\n\u001b[0;32m   1032\u001b[0m                         restore_signals,\n\u001b[0;32m   1033\u001b[0m                         gid, gids, uid, umask,\n\u001b[0;32m   1034\u001b[0m                         start_new_session, process_group)\n\u001b[0;32m   1035\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m   1036\u001b[0m     \u001b[39m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mfilter\u001b[39m(\u001b[39mNone\u001b[39;00m, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdin, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr)):\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\subprocess.py:1538\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[39m# Start the process\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1538\u001b[0m     hp, ht, pid, tid \u001b[39m=\u001b[39m _winapi\u001b[39m.\u001b[39;49mCreateProcess(executable, args,\n\u001b[0;32m   1539\u001b[0m                              \u001b[39m# no special security\u001b[39;49;00m\n\u001b[0;32m   1540\u001b[0m                              \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   1541\u001b[0m                              \u001b[39mint\u001b[39;49m(\u001b[39mnot\u001b[39;49;00m close_fds),\n\u001b[0;32m   1542\u001b[0m                              creationflags,\n\u001b[0;32m   1543\u001b[0m                              env,\n\u001b[0;32m   1544\u001b[0m                              cwd,\n\u001b[0;32m   1545\u001b[0m                              startupinfo)\n\u001b[0;32m   1546\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   1547\u001b[0m     \u001b[39m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1548\u001b[0m     \u001b[39m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[39m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m     \u001b[39m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   1553\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   1554\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   1555\u001b[0m                          errread, errwrite)\n","\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"]}]},{"cell_type":"code","source":"uri = \"mongodb+srv://iretioluwaolawuyi:7KoupjCPhO5uZwVC@cluster0.spixfoc.mongodb.net/?retryWrites=true&w=majority\"\n\ndf = spark.read\\\n.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n.option(\"uri\", uri)\\\n.option(\"database\", \"group_6_final_project\")\\\n.option(\"collection\", \"complaints\")\\\n.load()","metadata":{},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"https://stackoverflow.com/questions/58847634/pyspark-connection-to-the-microsoft-sql-server\n# Read from SQL Table\ndf_structured = spark.read \\\n  .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\n  .option(\"url\", \"jdbc:sqlserver://{SERVER_ADDR};databaseName=emp;\") \\\n  .option(\"dbtable\", \"employee\") \\\n  .option(\"user\", \"replace_user_name\") \\\n  .option(\"password\", \"replace_password\") \\\n  .load()\n\ndf_structured.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.show(3)","metadata":{},"execution_count":6,"outputs":[{"name":"stdout","output_type":"stream","text":"+--------------------+--------------------+-----------------------+--------------------+------------+-----------------------+-------------------------+-----------------+-------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+-------------+----+------+--------+\n\n|                 _id|             company|company_public_response|    company_response|complaint_id|complaint_what_happened|consumer_consent_provided|consumer_disputed|date_received|date_sent_to_company|               issue|             product|state|           sub_issue|         sub_product|submitted_via|tags|timely|zip_code|\n\n+--------------------+--------------------+-----------------------+--------------------+------------+-----------------------+-------------------------+-----------------+-------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+-------------+----+------+--------+\n\n|{64a0f68283dec217...|       EQUIFAX, INC.|                       |         In progress|     7129762|                       |                         |              N/A|   2023-06-17|          2023-06-17|Problem with a cr...|Credit reporting,...|   NY|Their investigati...|    Credit reporting|          Web|    |   Yes|   13902|\n\n|{64a0f68283dec217...|TRANSUNION INTERM...|                       |         In progress|     7129776|                       |                         |              N/A|   2023-06-17|          2023-06-17|Problem with a cr...|Credit reporting,...|   GA|Their investigati...|    Credit reporting|          Web|    |   Yes|   30096|\n\n|{64a0f68283dec217...|SECURITY SERVICE ...|   Company has respo...|Closed with monet...|     6638236|                       |                      N/A|              N/A|   2023-03-02|          2023-03-02|Problem when maki...|Payday loan, titl...|   TX|                    |Personal line of ...|        Phone|    |   Yes|   78214|\n\n+--------------------+--------------------+-----------------------+--------------------+------------+-----------------------+-------------------------+-----------------+-------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+-------------+----+------+--------+\n\nonly showing top 3 rows\n\n\n"}]},{"cell_type":"code","source":"print(\"The shape of the data is: \", df.count(), len(df.columns))","metadata":{},"execution_count":17,"outputs":[{"name":"stdout","output_type":"stream","text":"The shape of the data is:  115000 19\n"}]},{"cell_type":"code","source":"\n\ndf.select(\"company\").distinct().show(truncate=False)","metadata":{},"execution_count":15,"outputs":[{"name":"stdout","output_type":"stream","text":"+---------------------------------------------------+\n\n|company                                            |\n\n+---------------------------------------------------+\n\n|Block, Inc.                                        |\n\n|FORD MOTOR CREDIT CO.                              |\n\n|AFNI INC.                                          |\n\n|Medical Data Systems, Inc.                         |\n\n|Mr. Cooper Group Inc.                              |\n\n|Viking Client Services                             |\n\n|SinglePoint GI                                     |\n\n|OPORTUN FINANCIAL CORPORATION                      |\n\n|Auto Acceptance, LLC                               |\n\n|The Nguyen Law Firm, PLC                           |\n\n|Fidelity National Information Services, Inc. (FNIS)|\n\n|Selene Holdings LLC                                |\n\n|TEXAS CAPITAL BANCSHARES, INC.                     |\n\n|AMERIS BANCORP                                     |\n\n|Receivables Management Partners, LLC               |\n\n|LAKEVIEW LOAN SERVICING, LLC                       |\n\n|Capital Accounts, LLC                              |\n\n|Second Round Limited Partnership, Austin, TX Branch|\n\n|SECURITY SERVICE FEDERAL CREDIT UNION              |\n\n|GOLDMAN SACHS BANK USA                             |\n\n+---------------------------------------------------+\n\nonly showing top 20 rows\n\n\n"}]},{"cell_type":"code","source":"df.select(\"company\").distinct().count()","metadata":{},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":["1857"]},"metadata":{}}]},{"cell_type":"code","source":"df.groupBy(\"product\").agg(f.countDistinct(\"complaint_id\"), f.min(\"date_received\"),  f.max(\"date_received\")).show(truncate=False)\n","metadata":{},"execution_count":18,"outputs":[{"name":"stdout","output_type":"stream","text":"+----------------------------------------------------------------------------+-------------------+------------------+------------------+\n\n|product                                                                     |count(complaint_id)|min(date_received)|max(date_received)|\n\n+----------------------------------------------------------------------------+-------------------+------------------+------------------+\n\n|Bank account or service                                                     |2                  |2015-09-30        |2016-08-02        |\n\n|Checking or savings account                                                 |5423               |2017-11-20        |2023-06-16        |\n\n|Credit card                                                                 |1                  |2015-02-05        |2015-02-05        |\n\n|Credit card or prepaid card                                                 |5161               |2017-11-20        |2023-06-19        |\n\n|Credit reporting, credit repair services, or other personal consumer reports|86878              |2017-11-20        |2023-06-19        |\n\n|Debt collection                                                             |9497               |2014-02-14        |2023-06-19        |\n\n|Money transfer, virtual currency, or money service                          |1602               |2017-11-22        |2023-06-16        |\n\n|Mortgage                                                                    |3473               |2013-02-22        |2023-06-16        |\n\n|Payday loan, title loan, or personal loan                                   |844                |2017-11-21        |2023-06-18        |\n\n|Student loan                                                                |748                |2012-06-15        |2023-06-18        |\n\n|Vehicle loan or lease                                                       |1371               |2017-11-20        |2023-06-18        |\n\n+----------------------------------------------------------------------------+-------------------+------------------+------------------+\n\n\n"}]},{"cell_type":"code","source":"# Consumer Complaint - https://data.world/chandrasekar/consumer-complaints\n\nimport pandas as pd\n# structured_df = spark.read.load('https://query.data.world/s/bnfi533r3z4rkww443pzam5gf5sipc?dws=00000', format=\"csv\", header='true')\nstructured_pd = pd.read_csv('https://query.data.world/s/bnfi533r3z4rkww443pzam5gf5sipc?dws=00000')\nstructured_df = spark.createDataFrame(structured_pd)\nstructured_df.show(3)\n","metadata":{},"execution_count":10,"outputs":[{"name":"stdout","output_type":"stream","text":"+-------------+----------------+--------------------+--------------------+--------------------+----------------------------+-----------------------+--------------------+-----+--------+--------------+--------------------------+-------------+--------------------+----------------------------+----------------+------------------+------------+\n\n|Date received|         Product|         Sub-product|               Issue|           Sub-issue|Consumer complaint narrative|Company public response|             Company|State|ZIP code|          Tags|Consumer consent provided?|Submitted via|Date sent to company|Company response to consumer|Timely response?|Consumer disputed?|Complaint ID|\n\n+-------------+----------------+--------------------+--------------------+--------------------+----------------------------+-----------------------+--------------------+-----+--------+--------------+--------------------------+-------------+--------------------+----------------------------+----------------+------------------+------------+\n\n|   03/21/2017|Credit reporting|                 NaN|Incorrect informa...|Information is no...|                         NaN|   Company has respo...|EXPERIAN DELAWARE GP|   TX|   77075|Older American|                       NaN|        Phone|          03/21/2017|        Closed with non-m...|             Yes|                No|     2397100|\n\n|   04/19/2017| Debt collection|Other (i.e. phone...|Disclosure verifi...|Not disclosed as ...|                         NaN|                    NaN|Security Credit S...|   IL|   60643|           NaN|                       NaN|          Web|          04/20/2017|        Closed with expla...|             Yes|                No|     2441777|\n\n|   04/19/2017|     Credit card|                 NaN|               Other|                 NaN|                         NaN|   Company has respo...|      CITIBANK, N.A.|   IL|   62025|           NaN|                       NaN|     Referral|          04/20/2017|        Closed with expla...|             Yes|                No|     2441830|\n\n+-------------+----------------+--------------------+--------------------+--------------------+----------------------------+-----------------------+--------------------+-----+--------+--------------+--------------------------+-------------+--------------------+----------------------------+----------------+------------------+------------+\n\nonly showing top 3 rows\n\n\n"}]},{"cell_type":"code","source":"structured_df.describe().show()","metadata":{},"execution_count":13,"outputs":[{"name":"stdout","output_type":"stream","text":"+-------+-------------+--------------------+--------------------+--------------------+--------------------+----------------------------+-----------------------+--------------------+------+--------+-------------+--------------------------+-------------+--------------------+----------------------------+----------------+------------------+------------------+\n\n|summary|Date received|             Product|         Sub-product|               Issue|           Sub-issue|Consumer complaint narrative|Company public response|             Company| State|ZIP code|         Tags|Consumer consent provided?|Submitted via|Date sent to company|Company response to consumer|Timely response?|Consumer disputed?|      Complaint ID|\n\n+-------+-------------+--------------------+--------------------+--------------------+--------------------+----------------------------+-----------------------+--------------------+------+--------+-------------+--------------------------+-------------+--------------------+----------------------------+----------------+------------------+------------------+\n\n|  count|       777959|              777959|              777959|              777959|              777959|                      777959|                 777959|              777959|777959|  777959|       777959|                    777959|       777959|              777959|                      777959|          777959|            777959|            777959|\n\n|   mean|         null|                null|                 NaN|                null|                 NaN|                         NaN|                    NaN|                null|   NaN|     NaN|          NaN|                       NaN|         null|                null|                        null|            null|               NaN|1310412.9105801205|\n\n| stddev|         null|                null|                 NaN|                null|                 NaN|                         NaN|                    NaN|                null|   NaN|     NaN|          NaN|                       NaN|         null|                null|                        null|            null|               NaN| 728727.5794893295|\n\n|    min|   01/01/2012|Bank account or s...|(CD) Certificate ...|APR or interest rate|Account informati...|        ! ) forced placed...|   Company believes ...|(Former)Shapiro, ...|    AA|   (1158|          NaN|      Consent not provided|        Email|          01/01/2013|                      Closed|              No|               NaN|                 1|\n\n|    max|   12/31/2016|    Virtual currency|    Virtual currency|Wrong amount char...|You told them to ...|        ~ {$7000.00} in d...|                    NaN|reekside Recovery...|    WY|   `3290|Servicemember|                     Other|          Web|          12/31/2016|           Untimely response|             Yes|               Yes|           2488370|\n\n+-------+-------------+--------------------+--------------------+--------------------+--------------------+----------------------------+-----------------------+--------------------+------+--------+-------------+--------------------------+-------------+--------------------+----------------------------+----------------+------------------+------------------+\n\n\n"}]},{"cell_type":"code","source":"structured_df.summary().show()","metadata":{},"execution_count":12,"outputs":[{"name":"stdout","output_type":"stream","text":"+-------+-------------+--------------------+--------------------+--------------------+--------------------+----------------------------+-----------------------+--------------------+------+--------+-------------+--------------------------+-------------+--------------------+----------------------------+----------------+------------------+------------------+\n\n|summary|Date received|             Product|         Sub-product|               Issue|           Sub-issue|Consumer complaint narrative|Company public response|             Company| State|ZIP code|         Tags|Consumer consent provided?|Submitted via|Date sent to company|Company response to consumer|Timely response?|Consumer disputed?|      Complaint ID|\n\n+-------+-------------+--------------------+--------------------+--------------------+--------------------+----------------------------+-----------------------+--------------------+------+--------+-------------+--------------------------+-------------+--------------------+----------------------------+----------------+------------------+------------------+\n\n|  count|       777959|              777959|              777959|              777959|              777959|                      777959|                 777959|              777959|777959|  777959|       777959|                    777959|       777959|              777959|                      777959|          777959|            777959|            777959|\n\n|   mean|         null|                null|                 NaN|                null|                 NaN|                         NaN|                    NaN|                null|   NaN|     NaN|          NaN|                       NaN|         null|                null|                        null|            null|               NaN|1310412.9105801205|\n\n| stddev|         null|                null|                 NaN|                null|                 NaN|                         NaN|                    NaN|                null|   NaN|     NaN|          NaN|                       NaN|         null|                null|                        null|            null|               NaN| 728727.5794893295|\n\n|    min|   01/01/2012|Bank account or s...|(CD) Certificate ...|APR or interest rate|Account informati...|        ! ) forced placed...|   Company believes ...|(Former)Shapiro, ...|    AA|   (1158|          NaN|      Consent not provided|        Email|          01/01/2013|                      Closed|              No|               NaN|                 1|\n\n|    25%|         null|                null|                 NaN|                null|                 NaN|                         NaN|                    NaN|                null|   NaN| 22304.0|          NaN|                       NaN|         null|                null|                        null|            null|               NaN|            691196|\n\n|    50%|         null|                null|                 NaN|                null|                 NaN|                         NaN|                    NaN|                null|   NaN| 43231.0|          NaN|                       NaN|         null|                null|                        null|            null|               NaN|           1353247|\n\n|    75%|         null|                null|                 NaN|                null|                 NaN|                         NaN|                    NaN|                null|   NaN| 80205.0|          NaN|                       NaN|         null|                null|                        null|            null|               NaN|           1956785|\n\n|    max|   12/31/2016|    Virtual currency|    Virtual currency|Wrong amount char...|You told them to ...|        ~ {$7000.00} in d...|                    NaN|reekside Recovery...|    WY|   `3290|Servicemember|                     Other|          Web|          12/31/2016|           Untimely response|             Yes|               Yes|           2488370|\n\n+-------+-------------+--------------------+--------------------+--------------------+--------------------+----------------------------+-----------------------+--------------------+------+--------+-------------+--------------------------+-------------+--------------------+----------------------------+----------------+------------------+------------------+\n\n\n"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from datetime import datetime, date\nimport pandas as pd\nfrom pyspark.sql import Row\n\ndf = spark.createDataFrame([\n    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n])\ndf.show()","metadata":{},"execution_count":4,"outputs":[{"name":"stdout","output_type":"stream","text":"+---+---+-------+----------+-------------------+\n\n|  a|  b|      c|         d|                  e|\n\n+---+---+-------+----------+-------------------+\n\n|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n\n|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n\n|  4|5.0|string3|2000-03-01|2000-01-03 12:00:00|\n\n+---+---+-------+----------+-------------------+\n\n\n"}]},{"cell_type":"code","source":"pandas_df = pd.DataFrame({\n    'a': [1, 2, 3],\n    'b': [2., 3., 4.],\n    'c': ['string1', 'string2', 'string3'],\n    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n})\ndf = spark.createDataFrame(pandas_df)\ndf","metadata":{},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":["DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"]},"metadata":{}}]},{"cell_type":"code","source":"# All DataFrames above result same.\ndf.show()\ndf.printSchema()","metadata":{},"execution_count":5,"outputs":[{"name":"stdout","output_type":"stream","text":"+---+---+-------+----------+-------------------+\n\n|  a|  b|      c|         d|                  e|\n\n+---+---+-------+----------+-------------------+\n\n|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n\n|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n\n|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n\n+---+---+-------+----------+-------------------+\n\n\n\nroot\n\n |-- a: long (nullable = true)\n\n |-- b: double (nullable = true)\n\n |-- c: string (nullable = true)\n\n |-- d: date (nullable = true)\n\n |-- e: timestamp (nullable = true)\n\n\n"}]},{"cell_type":"code","source":"spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\ndf\n","metadata":{},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/html":["<table border='1'>\n","<tr><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr>\n","<tr><td>1</td><td>2.0</td><td>string1</td><td>2000-01-01</td><td>2000-01-01 12:00:00</td></tr>\n","<tr><td>2</td><td>3.0</td><td>string2</td><td>2000-02-01</td><td>2000-01-02 12:00:00</td></tr>\n","<tr><td>3</td><td>4.0</td><td>string3</td><td>2000-03-01</td><td>2000-01-03 12:00:00</td></tr>\n","</table>\n"],"text/plain":["DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"]},"metadata":{}}]},{"cell_type":"code","source":"df.collect()\n","metadata":{},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":["[Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0)),\n"," Row(a=2, b=3.0, c='string2', d=datetime.date(2000, 2, 1), e=datetime.datetime(2000, 1, 2, 12, 0)),\n"," Row(a=3, b=4.0, c='string3', d=datetime.date(2000, 3, 1), e=datetime.datetime(2000, 1, 3, 12, 0))]"]},"metadata":{}}]},{"cell_type":"code","source":"df.select(\"a\", \"b\", \"c\").describe().show()","metadata":{},"execution_count":8,"outputs":[{"name":"stdout","output_type":"stream","text":"+-------+---+---+-------+\n\n|summary|  a|  b|      c|\n\n+-------+---+---+-------+\n\n|  count|  3|  3|      3|\n\n|   mean|2.0|3.0|   null|\n\n| stddev|1.0|1.0|   null|\n\n|    min|  1|2.0|string1|\n\n|    max|  3|4.0|string3|\n\n+-------+---+---+-------+\n\n\n"}]}]}